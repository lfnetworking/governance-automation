# =============================================================================
#  LFN Project Governance Automation Workflow
#  --------------------------------------------------------------------------------
#  PURPOSE
#  -------
#  This workflow aims to provide consistency and engagement to the governance and health-check processes for Projects across all Linux Foundation Networking (LFN) projects. It regularly assesses repositories under LFN's umbrella to ensure continuous alignment with community health, security standards, and lifecycle progression defined by the Technical Advisory Council (TAC). This information will be helpful to the Members of the Board when making strategic decisions.
#  The timing of the assessment would depend on their lifecycle phase and the metrics being checked. The timing will need to be customizable by the TAC.

#  CORE OBJECTIVES
#  ---------------
#  • **Automated Repository Assessment:**
#    - Periodically scan every repository within the LFN umbrella projects (and induction-candidate repositories).
#    - Read existing health metrics, including commit activity, contributor engagement, release history, and lifecycle status.
#    - Initially designed to operate without requiring elevated permissions or special GitHub GraphQL scopes, ensuring secure, transparent, and community-friendly operation.

#  • **Lifecycle Classification:**
#    - Clearly categorize repositories into one of seven TAC-defined lifecycle phases:
#      1. Spark (Candidate/New)
#      2. Incubation
#      3. Active Development
#      4. Stable
#      5. Maintenance/Long-Term Support (LTS)
#      6. Archive
#      7. Inaccessible (permissions issue or repository not found)
#    - Provide a simple, community-readable summary for easy tracking and oversight by project maintainers, Program Managers (PMs), and the TAC.

#  • **Friendly and Encouraging Community Engagement:**
#    - Automatically create/update one GitHub issue per organization (project-level) with clear, friendly, and actionable next-step suggestions to improve their project's lifecycle status.
#    - Issues will not impose strict deadlines, emphasizing encouragement and constructive guidance, with recommended adjustable check-ins for progress updates.
#    - Repositories classified as "Archive" or "Inaccessible" are intentionally excluded from next-step issues as they have reached terminal phases.

#  FUTURE ENHANCEMENTS
#  -------------------
#  • **Security and Health:**
#    - Incorporate OSSF Scorecard checks
#    - Open tickets to the orgs (project-level) with helpful steps they can take to improve their current OSSF scorecard
#
#  • **Governance and Leadership:**
#    - Technical Steering Committee (TSC) membership is often submitted as part of Induction levels beyond Spark (and possibly even at Spark), but it is not always maintained in GitHub.  With community
#      agreement, it may be easier to maintain in GitHub for automation, if we can agree on a standard
#
#  • **Flexible TAC Lifecycle Adjustment:**
#    - Provide a clear, documented mechanism to modify lifecycle phase datapoints based on official TAC approvals (authentication of approvals currently outside workflow scope, maintained by LF staff or authorized community maintainers for now discuss with community).

#  READABILITY & MAINTAINABILITY
#  -----------------------------
#  This workflow is intentionally designed to be both verbose and structured for readability and ease of community collaboration. Community members are encouraged to contribute suggestions, enhancements, or adjustments to the logic to ensure continuous improvement and alignment with community goals.

name: "LFN Project Governance Automation"

on:
  # Allows manual triggering from the GitHub Actions UI ('Actions' tab -> select workflow -> 'Run workflow')
  workflow_dispatch:
    inputs:
      # Input to select the operational mode when manually triggered
      mode:
        description: "Choose the run mode: 'test' (runs on subset?), 'review' (standard run), 'induction' (runs on specific repo/org)"
        required: true
        default: "review" # Default to standard review mode
        type: choice
        options: [test, review, induction]
      # Input for the organization, used only in 'induction' mode
      project_org:
        description: "(induction) GitHub org where the new project lives"
        required: false # Only required if mode is 'induction'
      # Input for the repository slug, used only in 'induction' mode (if specifying single repo)
      project_repo:
        description: "(induction) Specific repository slug <org>/<repo> (optional, processes only this repo in the org)"
        required: false # Optional for induction mode

  # Schedule the workflow to run automatically
  schedule:
    # Runs at 09:00 UTC every Monday (adjust cron expression as needed)
    - cron: "0 9 * * 1"  # Example: 09:00 UTC Monday → 02:00 PT

# Define permissions required by the workflow at the top level
permissions:
  contents: read          # Needed to checkout code, read repo metadata, list repos
  issues: write           # Needed to create/update governance summary issues
  pull-requests: write    # Often needed by actions interacting with PRs (can potentially be reduced)
  id-token: write         # Needed if using OIDC for authentication (e.g., with external services, not currently used here but good practice if needed later)
  actions: write          # Needed for actions/upload-artifact

# Environment variables available to all jobs in the workflow
env:
  # Sets the GitHub token for all jobs. Using a specific LFN Admin Token for potentially elevated permissions if required across orgs.
  # Ensure this secret (LFN_ADMIN_TOKEN) is configured in the repository/organization settings and has appropriate scopes (e.g., 'repo').
  GITHUB_TOKEN: ${{ secrets.LFN_ADMIN_TOKEN }}
  PROJECT_CONFIG_PATH: '.lfn/projects.yaml'
  # --- PATH UPDATED ---
  CLASSIFY_CONFIG_PATH: '.lfn/classify-config.yml' # Path to the classification rules
  ISSUES_CONFIG_PATH: '.lfn/issues-config.yml' # Path to issue templates
  CLASSIFY_ACTION_PATH: './classify-config' # Path to the local JS action directory
  GOVERNANCE_ISSUE_TITLE: "Quarterly LFN Governance & Lifecycle Review" # Standard title for created issues


jobs:
  # =============================================================================
  # JOB 1: Enumerate Repositories
  # PURPOSE: Determines the full list of non-archived repositories across all
  #          relevant organizations that need to be processed.
  # INPUT:   Uses workflow inputs (mode, project_org, project_repo) and projects.yaml.
  # OUTPUT:  A JSON matrix ('matrix') where each element contains a repository slug
  #          in the format {"repo_slug": "org/repo"}.
  # =============================================================================
  enumerate-repos:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      # Step 1.1: Checkout the repository containing this workflow
      - name: Checkout Repository Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1 # Fetch only the latest commit for efficiency

      # Step 1.2: Install necessary command-line tools
      - name: Install yq and jq
        run: sudo apt-get update -y && sudo apt-get install -y yq jq

      # Step 1.3: Build the repository matrix
      - name: Build repo matrix
        id: set-matrix
        shell: bash
        env:
          MODE: ${{ github.event.inputs.mode || 'review' }}
          PROJECT_ORG: ${{ github.event.inputs.project_org }}
          PROJECT_REPO: ${{ github.event.inputs.project_repo }}
          GH_TOKEN: ${{ env.GITHUB_TOKEN }} # Pass token for gh cli
        run: |
          set -euo pipefail
          # set -x # Uncomment for extreme debugging

          declare -a org_list
          declare -a initial_repo_slug_list # Use a temporary name for initial collection
          declare all_repo_slugs_string="" # Temporary string to hold all slugs

          # --- Determine Target Orgs ---
          if [[ "$MODE" == "induction" ]]; then
            if [[ -n "$PROJECT_REPO" ]]; then
              # Induction mode with a specific repo slug
              echo "Induction mode: Processing single repo: $PROJECT_REPO" >&2
              # Basic validation
              if [[ "$PROJECT_REPO" != *"/"* ]]; then
                 echo "::error::Invalid project_repo format. Expected <org>/<repo>, got '$PROJECT_REPO'"
                 exit 1
              fi
              # Check if the specified repo name contains 'archived'
              repo_name_only=$(basename "$PROJECT_REPO")
              if [[ "$repo_name_only" == *archived* ]] || [[ "$repo_name_only" == *Archived* ]]; then # Case-insensitive check
                 echo "::warning::Skipping induction repo as its name contains 'archived': $PROJECT_REPO" >&2
              else
                 initial_repo_slug_list+=("$PROJECT_REPO") # Add to initial list
              fi
              # Skip org listing below if specific repo is given
              org_list=()
            elif [[ -n "$PROJECT_ORG" ]]; then
              # Induction mode with a specific org
              echo "Induction mode: Processing org: $PROJECT_ORG" >&2
              org_list+=("$PROJECT_ORG")
            else
              echo "::error::Induction mode requires either project_org or project_repo input." >&2
              exit 1
            fi
          else
            # Standard review/test mode: read orgs from config file
            echo "Review/Test mode: Reading orgs from ${{ env.PROJECT_CONFIG_PATH }}" >&2
            if [[ ! -f "${{ env.PROJECT_CONFIG_PATH }}" ]]; then echo "ERROR: ${{ env.PROJECT_CONFIG_PATH }} not found!" >&2; exit 1; fi
            mapfile -t org_list < <(yq -r '.orgs[] // ""' "${{ env.PROJECT_CONFIG_PATH }}" | grep -v '^$' )
            echo "Read ${#org_list[@]} orgs from config." >&2
          fi

          # --- List Repos for Each Target Org (if not single repo induction) ---
          if [[ ${#org_list[@]} -gt 0 ]]; then
             echo "Listing repositories for ${#org_list[@]} organizations..." >&2
             # Check gh auth
             if ! command -v gh &> /dev/null; then echo "ERROR: gh cli not found." >&2; exit 1; fi
             gh auth status || { echo "ERROR: gh auth status failed. Check token (LFN_ADMIN_TOKEN)." >&2; exit 1; }

             for org in "${org_list[@]}"; do
                echo "Listing repos for: $org" >&2
                # List repos, get names, filter out archived, prepend org name
                # Append to the string, separated by newline
                current_slugs=$( \
                   gh repo list "$org" --limit 1000 --json name --visibility=public -q '.' \
                   | jq -r '.[].name' \
                   | grep -vi 'archived' \
                   | sed "s|^|$org/|" \
                   || true # Prevent pipefail from exiting if grep finds nothing
                )
                if [[ -n "$current_slugs" ]]; then
                   all_repo_slugs_string+="$current_slugs"$'\n'
                fi
             done
          fi

          # Create temp file (even if empty) to avoid errors later
          touch temp_slugs.txt

          # Also add any initial slugs from single-repo induction mode (if the list is not empty)
          # This check is safe because the array is always declared
          if [[ ${#initial_repo_slug_list[@]} -gt 0 ]]; then
             printf '%s\n' "${initial_repo_slug_list[@]}" >> temp_slugs.txt
          fi

          # Add slugs collected from org loops
          if [[ -n "$all_repo_slugs_string" ]]; then
             # Use printf for safer handling of potential special characters
             printf '%s' "$all_repo_slugs_string" >> temp_slugs.txt
          fi

          # Populate the final array from the temp file
          declare -a repo_slug_list
          mapfile -t repo_slug_list < <(cat temp_slugs.txt | grep -v '^$') # Read non-empty lines into the array
          rm temp_slugs.txt
          # repo_slug_list is now populated or empty

          # --- Create Matrix ---
          total_repos=${#repo_slug_list[@]} # Access the length safely now
          echo "Total repositories to process: $total_repos" >&2

          if [ $total_repos -eq 0 ]; then
             echo "::warning::No repositories found to process after filtering."
             # Explicitly set empty matrix if no repos found
             final_matrix_json="{\"include\":[]}"
          else
             # Prepare the 'include' array for the matrix only if repos exist
             matrix_include_json=$(printf '%s\n' "${repo_slug_list[@]}" | jq -R '{repo_slug: .}' | jq -sc .)
             # Final matrix object structure
             final_matrix_json="{\"include\":${matrix_include_json}}"
          fi

          echo "matrix=${final_matrix_json}" >> "$GITHUB_OUTPUT"
          echo "Generated matrix for $total_repos repositories." >&2
          if [ $total_repos -gt 0 ]; then
             echo "Matrix structure preview: $(echo $final_matrix_json | jq -c . | cut -c 1-200)..." >&2
          else
             echo "Matrix is empty." >&2
          fi


  # =============================================================================
  # JOB 2: Classify Repository (Runs in Parallel per Repo)
  # PURPOSE: Runs the classification logic for a single repository.
  #          Uses a matrix strategy based on the repositories identified previously.
  # INPUT:   The 'matrix' output from 'enumerate-repos', containing repo slugs.
  # PROCESS: - For each repository job instance:
  #          - Checks out the main governance-automation repo.
  #          - Sets up Node.js and installs dependencies for the local action.
  #          - Clones the target repository slug.
  #          - Runs the classification Node.js script directly for that repo.
  #          - Saves the result to a uniquely named file (result-org--repo.json).
  #          - Uploads the single result file as a uniquely named artifact.
  # =============================================================================
  classify:
    needs: enumerate-repos # Depends on the 'enumerate-repos' job
    # Condition to prevent running if the matrix 'include' array is empty
    if: ${{ needs.enumerate-repos.outputs.matrix != '' && fromJson(needs.enumerate-repos.outputs.matrix).include[0] != null }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false # Allow other repos to process if one fails
      matrix: ${{ fromJson(needs.enumerate-repos.outputs.matrix) }} # Matrix iterates over repo objects: { "repo_slug": "org/repo" }
    steps:

      # Step 2.1: Checkout the main branch of this (governance-automation) repository.
      - name: Checkout governance-automation code for ${{ matrix.repo_slug }}
        uses: actions/checkout@v4
        with:
          ref: main

      # Step 2.2: Setup Node.js environment.
      - name: Setup Node.js for ${{ matrix.repo_slug }}
        uses: actions/setup-node@v4
        with:
          node-version: '16'

      # Step 2.3: Install dependencies for the local classification action.
      - name: Install Action Dependencies for ${{ matrix.repo_slug }}
        run: npm install
        working-directory: ${{ env.CLASSIFY_ACTION_PATH }}

      # Step 2.4: Process the single repository for this job instance.
      - name: Process Repository ${{ matrix.repo_slug }}
        id: process-repo # Give step an ID
        shell: bash
        env:
          # Pass the specific repo slug for this job instance
          REPO_SLUG: ${{ matrix.repo_slug }}
          CLASSIFY_CONFIG_FILE_PATH: ${{ env.CLASSIFY_CONFIG_PATH }}
          CLASSIFY_ACTION_DIR: ${{ env.CLASSIFY_ACTION_PATH }}
          # Explicitly set GH_TOKEN (uses LFN_ADMIN_TOKEN)
          GH_TOKEN: ${{ env.GITHUB_TOKEN }}
        run: |
          # Re-enable strict error checking
          set -euo pipefail
          # set -x # Uncomment for extreme debugging

          repo_slug="$REPO_SLUG"
          config_path="$CLASSIFY_CONFIG_FILE_PATH"
          action_dir="$CLASSIFY_ACTION_DIR"
          # Create a unique results directory for this job instance to avoid conflicts
          # Use a simpler name for the results directory, job runner ensures isolation
          results_dir="./classification-result"
          mkdir -p "$results_dir"

          target_repo_path="./repo_temp" # Local path for cloning
          safe_name=$(echo "$repo_slug" | sed 's|/|--|g')
          result_json_path="${results_dir}/${safe_name}.json" # Path for the final JSON output
          repo_failed=false

          # Clean up from potential previous runs (unlikely needed with unique dirs, but safe)
          rm -rf "$target_repo_path" || echo "::warning::Failed to remove $target_repo_path (continuing)"

          # --- Checkout target repo ---
          echo "Cloning $repo_slug..." >&2
          # Use a subshell and || true to capture output even on failure with set -e
          git_output=$( (git clone --depth 1 "https://github-actions:${GH_TOKEN}@github.com/${repo_slug}.git" "$target_repo_path") 2>&1 || true )
          git_exit_code=$? # Check the exit code after the command

          phase_value="Unknown"
          checkout_successful=false
          if [[ $git_exit_code -eq 0 ]]; then
            echo "Clone successful." >&2
            checkout_successful=true
          else
            echo "::error::Failed to clone $repo_slug. Exit code: $git_exit_code."
            echo "Git Output: $git_output"
            phase_value="Checkout Failed"
            repo_failed=true
            printf '{"phase": "%s", "error": "Checkout failed", "details": "%s"}\n' "$phase_value" "$(echo "$git_output" | jq -Rsa .)" > "$result_json_path"
          fi

          # --- Run Classification Script ---
          if [[ "$checkout_successful" == true ]]; then
            echo "Running classification script for $repo_slug..." >&2
            # Run node script
            INPUT_REPO="$repo_slug" \
            INPUT_CONFIG_PATH="$config_path" \
            GITHUB_TOKEN="${GH_TOKEN}" \
            node "${action_dir}/index.js" # This script should write repo.json
            node_exit_code=$?

            if [[ $node_exit_code -ne 0 ]]; then
               echo "::error::Classification script failed for $repo_slug with exit code $node_exit_code."
               phase_value="Classification Error"
               repo_failed=true
               printf '{"phase": "%s", "error": "Classification script failed", "exit_code": %s}\n' "$phase_value" "$node_exit_code" > "$result_json_path"
            elif [[ -f "repo.json" ]]; then
               # Move the generated repo.json to the final destination
               mv "repo.json" "$result_json_path" # Error handled by set -e
               echo "Classification script ran, result saved to $result_json_path" >&2
               phase_value=$(jq -r '.phase // "Unknown"' "$result_json_path" 2>/dev/null || echo "Read Error")
               if [[ "$phase_value" == "Read Error" ]]; then
                  echo "::error::Failed to read phase from generated $result_json_path for $repo_slug"
                  repo_failed=true
                  printf '{"phase": "%s", "error": "Failed to read phase from script output"}\n' "$phase_value" > "$result_json_path"
               fi
            else
               echo "::error::Classification script completed successfully but did not produce repo.json for $repo_slug"
               phase_value="Classification Error"
               repo_failed=true
               printf '{"phase": "%s", "error": "Classification script did not produce output file"}\n' "$phase_value" > "$result_json_path"
            fi
          fi # end if checkout_successful

          echo "Final phase for $repo_slug: $phase_value" >&2

          # Clean up repo clone
          rm -rf "$target_repo_path" || echo "::warning::Failed to remove $target_repo_path (continuing)"
          echo "--- Finished processing repository: $repo_slug ---" >&2

          # If repo failed, ensure the script exits non-zero to potentially mark the job as failed/unstable
          if [[ "$repo_failed" == true ]]; then
             echo "::error::Processing failed for repository $repo_slug."
             # exit 1 # Optionally exit non-zero if a single repo failure should fail the job
          fi

      # Step 2.5: Upload the single result file as an artifact for this repo job
      - name: Upload Classification Artifact for ${{ matrix.repo_slug }}
        uses: actions/upload-artifact@v4
        if: always() # Upload result even if script failed
        with:
          # Create a unique artifact name for each repo result using a safe version of the slug
          # Replace / with -- for artifact name compatibility
          name: result-${{ matrix.repo_slug }}
          # Path to the specific JSON file created in the previous step
          # Use the safe_name variable which was already calculated
          path: ./classification-result/${{ matrix.repo_slug }}.json
          retention-days: 5 # Optional: Adjust retention


  # =============================================================================
  # JOB 3: Publish Report
  # PURPOSE: Aggregates the classification results from all repositories (across
  #          all orgs/jobs), generates a Markdown summary report, and posts/updates
  #          that report as a GitHub issue in the main governance-automation repository.
  # INPUT:   Artifacts uploaded by the 'classify' job instances (one per repo).
  # PROCESS: - Downloads all artifacts (each containing one JSON file).
  #          - Parses each downloaded JSON file to get the repo and phase.
  #          - Builds the Markdown report ('summary.md') as before.
  #          - Uses github-script to find/create/update the summary issue.
  #          - Optionally notifies Slack.
  # =============================================================================
  publish-report:
    needs: classify # Depends on the completion of all 'classify' jobs
    runs-on: ubuntu-latest
    if: always() # Run even if some classify jobs failed
    steps:
      # Step 3.1: Download all classification result artifacts.
      # Uses download-artifact v4's ability to download multiple artifacts by pattern.
      - name: Download classification artifacts
        uses: actions/download-artifact@v4
        with:
          # Download all artifacts produced by the classify jobs into the 'artifacts' directory
          # Each artifact will be in a subdirectory named after the artifact name (result-org--repo)
          path: artifacts
          pattern: result-* # Match all artifacts starting with "result-"
          merge-multiple: true # Download all matching artifacts into the same 'artifacts' directory

      # Step 3.1.1: DEBUG - List downloaded files
      - name: List downloaded files
        if: always()
        run: |
          echo "Listing contents of ./artifacts directory:"
          ls -R artifacts || echo "Artifacts directory not found or empty."
          echo "-----------------------------------------"

      # Step 3.2: Build the Markdown Summary Report.
      - name: Build Markdown Report
        id: build-md
        shell: bash
        run: |
          set -euo pipefail
          if ! command -v jq &> /dev/null; then
            echo "jq not found, installing..." >&2
            sudo apt-get update -y && sudo apt-get install -y jq
          fi

          declare -A rows_data
          echo "Scanning artifacts directory: $(pwd)/artifacts" >&2

          # Check if artifacts directory exists and has content
          if [ ! -d "artifacts" ] || [ -z "$(ls -A artifacts)" ]; then
              echo "WARNING: Artifacts directory is missing or empty. No artifacts to process."
              # Create an empty report if no artifacts were downloaded
              echo "# LFN Project Lifecycle Summary" > summary.md
              echo "" >> summary.md
              echo "*No artifacts found. Check 'classify' job logs for errors (e.g., artifact upload failures).*" >> summary.md
          else
              # Find all downloaded JSON files (*.json) directly within the artifacts directory
              # (download-artifact with merge-multiple puts all files in the target path)
              while IFS= read -r -d $'\0' file; do
                # Check if the found item is actually a file before processing
                if [[ ! -f "$file" ]]; then
                    echo "Warning: Skipping non-file item found by find: $file" >&2
                    continue
                fi

                # Extract org--repo from filename (e.g., onap--cps.json)
                artifact_name_with_ext=$(basename "$file")
                artifact_name=${artifact_name_with_ext%.json}

                # Convert safe name back to org/repo slug
                repo_slug=${artifact_name//--/\/}
                project=${repo_slug%%/*}
                repo=${repo_slug#*/}

                # Attempt to read phase, handle potential errors during read/parse
                phase=$(jq -r '.phase // "Unknown"' "$file" 2>/dev/null)
                jq_exit_code=$?
                # Check jq exit code and if phase is null/empty string
                if [[ $jq_exit_code -ne 0 ]] || [[ -z "$phase" ]] || [[ "$phase" == "null" ]]; then
                  # If jq failed or phase is empty/null, try reading error field
                  error_msg=$(jq -r '.error // ""' "$file" 2>/dev/null)
                  if [[ -n "$error_msg" ]]; then
                     # Sanitize error message for Markdown table (optional)
                     # error_msg=$(echo "$error_msg" | sed 's/|/\\|/g')
                     phase="Error: $error_msg" # Use error message as phase
                  else
                     phase="Unknown (Parse Error)" # Fallback if no error field
                  fi
                  echo "Warning: Could not parse phase or phase was null/empty in $file ($artifact_name). Setting phase to '$phase'." >&2
                fi

                row_line="| [$repo](https://github.com/$repo_slug) | $phase |" # Link uses full slug now
                if [[ -z "${rows_data[$project]:-}" ]]; then
                  rows_data[$project]="$row_line"
                else
                  rows_data[$project]+=$'\n'"$row_line"
                fi
              # Find *.json files directly within the artifacts/ directory
              done < <(find artifacts -maxdepth 1 -name '*.json' -print0)


              if [ ${#rows_data[@]} -eq 0 ]; then
                echo "WARNING: No valid artifact data found to generate report, although artifacts directory exists and files were found." >&2
                echo "# LFN Project Lifecycle Summary" > summary.md
                echo "" >> summary.md
                echo "*No project data processed successfully. Check the 'classify' job logs and artifact contents for errors.*" >> summary.md
              else
                {
                  echo "# LFN Project Lifecycle Summary"
                  echo ""
                  echo "Generated on: $(date -u)"
                  echo "Workflow Run: <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}>"
                  echo ""
                  sorted_projects=()
                  while IFS= read -r key; do
                      sorted_projects+=("$key")
                  done < <(printf '%s\n' "${!rows_data[@]}" | sort)

                  for project in "${sorted_projects[@]}"; do
                    echo
                    echo "## $project"
                    echo "| Repository | Phase |"
                    echo "|------------|-------|"
                    # Sort rows: Errors/Unknown first, then by phase, then by repo name
                    mapfile -t sorted_lines < <(printf '%s\n' "${rows_data[$project]}" | awk -F'|' '{gsub(/^[ \t]+|[ \t]+$/, "", $3); if ($3 ~ /^Error|^Unknown|^Checkout Failed|^Classification Error/) print "0"$0; else print "1"$0 }' | sort -t'|' -k1,1 -k3,3 -k2,2 | sed 's/^.//')
                    printf '%s\n' "${sorted_lines[@]}"
                  done
                } > summary.md
              fi
          fi # End check for artifacts directory

          echo "Generated summary.md:" >&2
          cat summary.md >&2

          {
            echo 'summary<<EOF'
            cat summary.md
            echo 'EOF'
          } >> "$GITHUB_OUTPUT"

          cat summary.md >> "$GITHUB_STEP_SUMMARY"

      # Step 3.3: Create or Update the Governance Summary Issue
      - name: Create or update governance issue
        uses: actions/github-script@v7
        with:
          github-token: ${{ env.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            // Note: 'core', 'github', and 'context' are automatically provided by actions/github-script@v7
            let body = "Failed to read summary.md";
            if (fs.existsSync('summary.md')) {
              body = fs.readFileSync('summary.md', 'utf8');
            } else {
              core.warning("summary.md not found for issue creation/update.");
              body = "Error: Workflow failed to generate summary report.";
            }
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const title = 'Automated LFN Project Lifecycle Summary';
            const token = process.env.GITHUB_TOKEN;
            if (!token) {
              core.setFailed('GITHUB_TOKEN not found in environment.');
              return;
            }
            // Use the pre-authenticated 'github' object provided by the action
            try {
              const issues = await github.paginate(github.rest.issues.listForRepo, {
                owner, repo, labels: 'lfn-governance', state: 'open'
              });
              const existing = issues.find(i => i.title === title);
              if (existing) {
                core.info(`Found existing issue #${existing.number}. Updating...`);
                await github.rest.issues.update({ owner, repo, issue_number: existing.number, body });
                core.info(`Updated issue #${existing.number}`);
              } else {
                core.info("No existing governance issue found. Creating new issue...");
                await github.rest.issues.create({ owner, repo, title, body, labels: ['lfn-governance'] });
                core.info('Created new governance issue');
              }
            } catch (error) {
              core.setFailed(`Failed to create or update governance issue: ${error.message}`);
              console.error("Error details:", error);
            }

      # Step 3.4: Optional Slack Notification
      - name: Notify Slack (optional)
        if: ${{ env.SLACK_WEBHOOK_URL != '' }}
        uses: slackapi/slack-github-action@v1.24.0
        with:
          payload: |
            {
              "text": ":clipboard: LFN governance summary updated for *${{ github.repository }}* – <${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|view run>"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
